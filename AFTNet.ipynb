{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":579020,"sourceType":"datasetVersion","datasetId":270005},{"sourceId":1118216,"sourceType":"datasetVersion","datasetId":627736},{"sourceId":3111719,"sourceType":"datasetVersion","datasetId":1899282},{"sourceId":3241736,"sourceType":"datasetVersion","datasetId":1964876},{"sourceId":3879819,"sourceType":"datasetVersion","datasetId":2305649},{"sourceId":5251537,"sourceType":"datasetVersion","datasetId":3055596}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install segmentation-models-pytorch\n!pip install pytorch-msssim\n!pip install ptflops\n!pip install lpips\n!pip install --upgrade pip\n!pip install torch\n!pip install cv2\n!pip install scikit-image\n!pip install albumentations\n!pip install einops\n!pip install wandb\n!pip install torchmetrics\n!pip install pyiqa\n!pip install pytorch_fid\n!pip install piqa\n\n#!apt-get update && apt-get install libgl1 -y\n#!apt-get update && apt-get install -y python3-opencv\n#!pip install opencv-python\nfrom torch.nn.utils import spectral_norm\nfrom pytorch_msssim import ms_ssim, ssim, ssim as f_ssim\nfrom ptflops import get_model_complexity_info\nimport segmentation_models_pytorch as smp\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport math\nimport torch.nn.init as init\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import CIFAR10, CIFAR100\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.utils import save_image, make_grid\nfrom torchvision.models import resnet50, densenet121, DenseNet121_Weights, ResNet50_Weights\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport sys\nimport gc\nimport numpy as np\nimport torchvision\nfrom tqdm import tqdm\nfrom skimage.metrics import mean_squared_error as f_mse\nfrom skimage.metrics import peak_signal_noise_ratio as f_psnr\nfrom skimage.metrics import structural_similarity as f_ssim\nfrom skimage.metrics import normalized_root_mse as f_nrmse\nfrom skimage.metrics import normalized_mutual_information as f_nmi\nfrom torch.cuda.amp import autocast, GradScaler\nfrom PIL import Image\nimport cv2\nfrom torchvision.models import vgg19, VGG19_Weights, VGG16_Weights\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom einops import rearrange, repeat\n","metadata":{"id":"vIoU2OFzMgG8","outputId":"4191b285-5365-4ff3-edf2-1884d53ed86c","trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"5yE9i11sMgHJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom einops import rearrange, reduce\nfrom torch.nn.parameter import Parameter\n\nclass AdaptiveFeatureNorm(nn.Module):\n    \"\"\"Novel Adaptive Feature Normalization module with learnable statistics\"\"\"\n    def __init__(self, num_features, eps=1e-5):\n        super(AdaptiveFeatureNorm, self).__init__()\n        self.eps   = eps\n        self.gamma = Parameter(torch.ones(1, num_features, 1, 1))\n        self.beta  = Parameter(torch.zeros(1, num_features, 1, 1))\n        \n        # Adaptive statistics network\n        self.stats_net = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(num_features, num_features//4, 1),\n            nn.ReLU(True),\n            nn.Conv2d(num_features//4, num_features*2, 1)\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        \n        # Calculate adaptive statistics\n        stats = self.stats_net(x)\n        adaptive_gamma, adaptive_beta = torch.chunk(stats, 2, dim=1)\n        \n        # Instance normalization\n        var, mean = torch.var_mean(x, dim=(2, 3), keepdim=True)\n        x_norm = (x - mean) / (var + self.eps).sqrt()\n        \n        # Apply adaptive scaling and shifting\n        return (1 + adaptive_gamma) * self.gamma * x_norm + adaptive_beta * self.beta\n\nclass MultiScaleFrequencyAttention(nn.Module):\n    \"\"\"Novel multi-scale frequency attention module\"\"\"\n    def __init__(self, dim, num_heads=8):\n        super(MultiScaleFrequencyAttention, self).__init__()\n        self.num_heads = num_heads\n        self.scale = dim ** -0.5\n        \n        self.qkv  = nn.Conv2d(dim, dim*3, 1)\n        self.proj = nn.Conv2d(dim, dim, 1)\n        \n        # Frequency decomposition branches\n        self.freq_decomp = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(dim, dim//4, 3, padding=1, groups=dim//4),\n                nn.GELU(),\n                nn.Conv2d(dim//4, dim, 1)\n            ) for _ in range(3)  # Low, mid, high frequencies\n        ])\n        \n        # Frequency attention weights\n        self.freq_weights = nn.Parameter(torch.ones(3))\n        self.softmax      = nn.Softmax(dim=0)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        \n        # Multi-head attention\n        qkv = self.qkv(x).reshape(B, 3, self.num_heads, C // self.num_heads, H, W)\n        q, k, v = qkv.unbind(1)\n        \n        # Attention computation\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        \n        # Frequency decomposition\n        freq_components = [decomp(x) for decomp in self.freq_decomp]\n        freq_weights = self.softmax(self.freq_weights)\n        \n        # Combine frequency components\n        freq_out = sum([w * f for w, f in zip(freq_weights, freq_components)])\n        \n        # Combine with spatial attention\n        x = (attn @ v).transpose(1, 2).reshape(B, C, H, W)\n        x = self.proj(x)\n        \n        return x + freq_out\n\nclass TemporalConsistencyModule(nn.Module):\n    \"\"\"Novel temporal consistency module with adaptive feature alignment\"\"\"\n    def __init__(self, dim):\n        super(TemporalConsistencyModule, self).__init__()\n        \n        # Feature alignment network\n        self.alignment_net = nn.Sequential(\n            nn.Conv2d(dim*2, dim//2, 3, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(dim//2, 2, 3, padding=1)\n        )\n        \n        # Temporal attention\n        self.temporal_attn = nn.Sequential(\n            nn.Conv2d(dim*2, dim//2, 1),\n            nn.ReLU(True),\n            nn.Conv2d(dim//2, dim, 1),\n            nn.Sigmoid()\n        )\n        \n        # Feature fusion\n        self.fusion = nn.Conv2d(dim*2, dim, 1)\n\n    def forward(self, current, previous):\n        # Calculate optical flow\n        flow = self.alignment_net(torch.cat([current, previous], dim=1))\n        \n        # Warp previous features\n        grid = self.get_grid(flow)\n        warped_prev = F.grid_sample(previous, grid, align_corners=True)\n        \n        # Temporal attention\n        attn = self.temporal_attn(torch.cat([current, warped_prev], dim=1))\n        \n        # Feature fusion\n        fused = self.fusion(torch.cat([current * attn, warped_prev * (1-attn)], dim=1))\n        return fused.float()\n\n    def get_grid(self, flow):\n        B, _, H, W = flow.size()\n        xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n        yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n        xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n        yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n        grid = torch.cat((xx,yy),1).float().to(flow.device)\n        vgrid = grid + flow\n        \n        # Scale grid to [-1,1]\n        vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:].clone()/max(W-1,1)-1.0\n        vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:].clone()/max(H-1,1)-1.0\n        return vgrid.permute(0,2,3,1)\n\nclass AdaptiveResidualBlock(nn.Module):\n    \"\"\"Novel adaptive residual block with dynamic routing\"\"\"\n    def __init__(self, dim):\n        super(AdaptiveResidualBlock, self).__init__()\n        \n        self.branch1 = nn.Sequential(\n            nn.Conv2d(dim, dim//4, 3, padding=1),\n            AdaptiveFeatureNorm(dim//4),\n            nn.GELU(),\n            nn.Conv2d(dim//4, dim, 3, padding=1),\n            AdaptiveFeatureNorm(dim)\n        )\n        \n        self.branch2 = MultiScaleFrequencyAttention(dim)\n        \n        # Dynamic routing network\n        self.router = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(dim, 2, 1),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        route_weights = self.router(x)\n        out1 = self.branch1(x)\n        out2 = self.branch2(x)\n        \n        return x + route_weights[:,0:1,:,:] * out1 + route_weights[:,1:2,:,:] * out2\n\ndef adjust(x1, x2):\n    x1 = F.interpolate(x1, size=x2.shape[2:], mode='nearest')\n    return x1\n\nclass AFTNet(nn.Module):\n    \"\"\"Advanced Adaptive Frequency-Temporal Network for Image Deblurring\"\"\"\n    def __init__(self, in_channels=3, dim=32, num_blocks=4):\n        super(AFTNet, self).__init__()\n        \n        # Initial feature extraction\n        self.init_conv = nn.Sequential(\n            nn.Conv2d(in_channels, dim, 3, padding=1),\n            AdaptiveFeatureNorm(dim)\n        )\n        \n        # Encoder\n        self.encoder = nn.ModuleList([\n            nn.Sequential(\n                AdaptiveResidualBlock(dim * (2**i)),\n                nn.Conv2d(dim * (2**i), dim * (2**(i+1)), 2, stride=2),\n                AdaptiveFeatureNorm(dim * (2**(i+1)))\n            ) for i in range(3)\n        ])\n\n        # bringing out prev feature to same level as the middle\n        self.conv_middle = nn.Conv2d(dim, dim*8, 1)\n        \n        # Middle blocks with temporal consistency\n        self.middle = nn.ModuleList([\n            nn.Sequential(\n                AdaptiveResidualBlock(dim * 8),\n                TemporalConsistencyModule(dim * 8)\n            ) for _ in range(num_blocks)\n        ])\n        \n        # Decoder\n        self.decoder = nn.ModuleList([\n            nn.Sequential(\n                nn.ConvTranspose2d(dim * (2**(i+1)), dim * (2**i), 2, stride=2),\n                AdaptiveFeatureNorm(dim * (2**i)),\n                AdaptiveResidualBlock(dim * (2**i))\n            ) for i in range(2, -1, -1)\n        ])\n        \n        # Feature pyramid fusion\n        self.pyramid_fusion = nn.ModuleList([\n            nn.Conv2d(dim * (2**i) * 2, dim * (2**i), 1)\n            for i in range(3)\n        ])\n\n        # Multi-scale output\n        self.output_layers = nn.ModuleList([\n            nn.Conv2d(dim * (2**i), in_channels, 3, padding=1)\n            for i in range(4)\n        ])\n\n    def forward(self, x, prev_frame=None):\n        r = x\n        if prev_frame is None:\n            prev_frame = x\n            \n        # Initial features\n        x = self.init_conv(x)\n        with torch.no_grad():\n            prev_features = self.init_conv(prev_frame)\n        \n        # Encoder\n        encoder_features = [x]\n        for enc in self.encoder:\n            x = enc(x)\n            encoder_features.append(x)\n\n        prev_features = F.interpolate(prev_features, size=x.shape[2:], mode='bicubic', align_corners=False)\n        prev_features = self.conv_middle(prev_features)\n\n        # Middle blocks with temporal consistency\n        for block in self.middle:\n            x = block[0](x)                 # Residual block\n            x = block[1](x, prev_features)  # Temporal consistency\n            prev_features = x\n\n        # Multi-scale outputs\n        outputs = [(self.output_layers[-1](x)+F.interpolate(r, size=x.shape[2:], mode='bicubic', align_corners=False)).clamp(0, 1)] \n        \n        # Decoder with feature pyramid fusion\n        for i, dec in enumerate(self.decoder):\n            # Upsample current features\n            x = dec[0](x)  # Upsample\n            \n            # Fusion with encoder features\n            s = encoder_features[::-1][1:][i]\n            x = adjust(x, s)\n            x = self.pyramid_fusion[::-1][i](torch.cat([x, s], dim=1))\n            \n            # Apply remaining decoder operations\n            x = dec[1:](x).float()\n            \n            # Generate output at current scale\n            outputs.append((self.output_layers[-(i+2)](x)+F.interpolate(r, size=x.shape[2:], mode='bicubic', align_corners=False)).clamp(0, 1))\n        \n        return outputs[::-1]  # Return multi-scale outputs from fine to coarse\n","metadata":{"id":"AP98myAnMgHL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Discriminator","metadata":{"id":"mTgjyCmbMgHP"}},{"cell_type":"code","source":"# models/discriminator.py\nclass Discriminator(nn.Module):\n    def __init__(self, input_nc=3, ndf=64, n_layers=3):\n        super().__init__()\n\n        model = [\n            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        for i in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**i, 8)\n            model += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        model += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=1, padding=1),\n            nn.BatchNorm2d(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n        ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"25bASKFlMgHS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Losses","metadata":{"id":"k7dfD2MyQYS_"}},{"cell_type":"code","source":"from torch.autograd import Variable\nfrom torch.fft import fft2, ifft2\n\nclass CharbonnierLoss(nn.Module):\n    def __init__(self, epsilon=1e-3):\n        super().__init__()\n        self.epsilon = epsilon\n\n    def forward(self, pred, target):\n        diff = pred - target\n        loss = torch.mean(Variable(torch.sqrt(diff * diff + self.epsilon * self.epsilon).type(torch.FloatTensor), requires_grad=True))\n        return loss\n\nclass MSEGDL(nn.Module):\n    def __init__(self, lambda_mse=1, lambda_gdl=1):\n        super(MSEGDL, self).__init__()\n        self.lambda_mse = lambda_mse\n        self.lambda_gdl = lambda_gdl\n\n    def forward(self, inputs, targets):\n\n        squared_error = (inputs - targets).pow(2)\n        gradient_diff_i = (inputs.diff(axis=-1)-targets.diff(axis=-1)).pow(2)\n        gradient_diff_j =  (inputs.diff(axis=-2)-targets.diff(axis=-2)).pow(2)\n        loss = (self.lambda_mse*squared_error.sum() + self.lambda_gdl*gradient_diff_i.sum() + self.lambda_gdl*gradient_diff_j.sum())/inputs.numel()\n\n        return loss\n\nclass SSIMLoss(nn.Module):\n    def __init__(self, data_range=1.0, size_average=True):\n        super(SSIMLoss, self).__init__()\n        self.data_range = data_range\n        self.size_average = size_average\n\n    def forward(self, img1, img2):\n        return 1 - Variable(ssim(img1, img2, data_range=self.data_range, size_average=self.size_average).type(torch.FloatTensor), requires_grad=True)\n\nclass MSSSIMLoss(nn.Module):\n    def __init__(self, data_range=1.0, size_average=True):\n        super(MSSSIMLoss, self).__init__()\n        self.data_range = data_range\n        self.size_average = size_average\n\n    def forward(self, img1, img2):\n        return 1 - Variable(ms_ssim(img1, img2, data_range=self.data_range, size_average=self.size_average).type(torch.FloatTensor), requires_grad=True)\n\nclass VGGLoss(nn.Module):\n    def __init__(self, layer=36):\n        super().__init__()\n\n        self.vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:layer].eval()\n        self.loss = nn.MSELoss()\n\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n\n    def forward(self, output, target):\n        self.vgg.eval()\n        vgg_input_features = self.vgg(output)\n        vgg_target_features = self.vgg(target)\n        loss = self.loss(vgg_input_features, vgg_target_features)\n        del vgg_input_features, vgg_target_features\n        gc.collect()\n        torch.cuda.empty_cache()\n        return loss\n\nclass DeblurLoss(nn.Module):\n    \"\"\"Advanced loss function combining multiple objectives\"\"\"\n    def __init__(self):\n        super(DeblurLoss, self).__init__()\n        self.l1_loss   = nn.L1Loss()\n        self.mse_loss  = nn.MSELoss()\n        self.gdl_loss  = MSEGDL()\n        self.ssim_loss = SSIMLoss()\n        self.vgg_loss  = VGGLoss()\n            \n    def get_frequency_loss(self, pred, target):\n        # FFT-based frequency loss\n        pred_freq = torch.fft.fft2(pred)\n        target_freq = torch.fft.fft2(target)\n        return F.mse_loss(pred_freq.abs(), target_freq.abs())\n\n    def forward(self, pred_list, target):\n        total_loss = 0\n        weights = [1.0, 0.75, 0.45, 0.35]  # Weights for different scales\n        \n        for pred, weight in zip(pred_list, weights):\n            # Resize target to match prediction if needed\n            if pred.shape != target.shape:\n                target_resized = F.interpolate(target, size=pred.shape[2:]).to(target)\n            else:\n                target_resized = target\n\n            pred = pred.to(target)\n            # Pixel loss\n            pixel_loss = self.l1_loss(pred, target_resized)\n            \n            # Frequency loss\n            freq_loss = self.get_frequency_loss(pred, target_resized)\n            \n            # Perceptual loss\n            perc_loss = self.vgg_loss(pred, target_resized)\n\n            # SSIM loss\n            ssim_loss = self.ssim_loss(pred, target_resized)\n\n            # GDL loss\n            gdl_loss = self.gdl_loss(pred, target_resized)\n            \n            # Combine losses with weights\n            total_loss += weight * (\n                1.0 * pixel_loss + \n                0.6 * ssim_loss +\n                0.3 * gdl_loss +\n                0.1 * freq_loss + \n                0.8 * perc_loss\n            )\n            \n        return total_loss","metadata":{"id":"XoG5ZrDwQa2z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utilities","metadata":{"id":"XZX9jDncMgHU"}},{"cell_type":"code","source":"import random\n\n# Training and Validation Functions\ndef calculate_metrics(pred, target):\n    \"\"\"Calculate PSNR and SSIM metrics\"\"\"\n    mse = F.mse_loss(pred, target)\n    psnr = 10 * torch.log10(1 / mse)\n    ssim_value = ssim(pred, target, data_range=1.0, size_average=True)\n    return psnr.item(), ssim_value.item()\n\ndef get_model_size(model):\n    \"\"\"\n    Calculates the size of a PyTorch model in megabytes (MB).\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to calculate the size for.\n\n    Returns:\n        float: The size of the model in megabytes (MB).\n    \"\"\"\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n\n    total_size = (param_size + buffer_size) / 1024**2\n    return total_size\n\ndef plot_dataset(train_loader):\n    fig, axes = plt.subplots(2, 5, figsize=(14, 7))\n\n    for i, (low_res, high_res) in enumerate(train_loader):\n        if i >= 5:\n            plt.show()\n            break\n\n        axes[0, i].imshow(low_res[0].permute(1, 2, 0))\n        axes[0, i].set_title(\"Low Resolution\")\n        axes[0, i].axis('off')\n\n        axes[1, i].imshow(high_res[0].permute(1, 2, 0))\n        axes[1, i].set_title(\"High Resolution\")\n        axes[1, i].axis('off')\n\ndef get_pil_image(image_tensor):\n    transform = transforms.Compose([\n        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n        transforms.Lambda(lambda t: t*255.),\n        transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n        transforms.ToPILImage()\n    ])\n    return transform(image_tensor)\n\ndef save_image_tensor(tensor_image, image_name):\n  # Convert the tensor image to a PIL image\n  pil_image = get_pil_image(tensor_image.squeeze(0))\n  # Save the PIL image\n  pil_image.save(image_name)\n\ndef save_pil_image(image, image_name):\n    image.save(image_name)\n\ndef save_samples(encoder, real_images, sharp_images, index, sample_dir='generated', show=True, device='cuda'):\n  with torch.no_grad():\n    #Sample random style code\n    fake_images = encoder(real_images)[0]\n    fake_name   = \"generated-images-{0:0=4d}.png\".format(index)\n    save_image(fake_images, os.path.join(sample_dir, fake_name), nrow=8)\n    if show:\n        fig, ax = plt.subplots(figsize=(20, 20))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n        plt.show()\n        fig, ax = plt.subplots(figsize=(20, 20))\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(make_grid(sharp_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n        plt.show()\n\ndef show_images(images):\n    fig, ax = plt.subplots(figsize=(20, 20))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(images.cpu().detach(), nrow=8).permute(1, 2, 0))\n    plt.show()\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\ndef get_model_size(model):\n    \"\"\"\n    Calculates the size of a PyTorch model in megabytes (MB).\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to calculate the size for.\n\n    Returns:\n        float: The size of the model in megabytes (MB).\n    \"\"\"\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n\n    total_size = (param_size + buffer_size) / 1024**2\n    return total_size","metadata":{"id":"-OGwgwveMgHV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Datasets","metadata":{"id":"dEI7v2pVMgHY"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom pathlib import Path\nfrom PIL import Image, ImageFile\nimport torchvision.transforms as transforms\nimport random\nimport numpy as np\nimport os\n\n# Allow loading of truncated images\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass DeblurDataset(Dataset):\n    def __init__(self, root_dir, split='train', patch_size=256):\n        \"\"\"\n        Dataset for deblurring training/validation with support for multiple image extensions\n        \n        Args:\n            root_dir: Root directory containing 'sharp' and 'blur' subdirectories\n            split: 'train' or 'val'\n            patch_size: Size of training patches (only used during training)\n        \"\"\"\n        self.root_dir = Path(root_dir)\n        self.split = split\n        self.patch_size = patch_size\n        \n        # Get sharp images with multiple extensions\n        self.sharp_dir = self.root_dir / 'sharp'\n        self.blur_dir = self.root_dir / 'blur'\n        \n        # Common image extensions\n        self.extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp']\n        \n        # Get all sharp images\n        self.sharp_files = []\n        for ext in self.extensions:\n            self.sharp_files.extend(list(self.sharp_dir.glob(f'*{ext}')))\n            self.sharp_files.extend(list(self.sharp_dir.glob(f'*{ext.upper()}')))\n        \n        # Sort the files to ensure deterministic behavior\n        self.sharp_files = sorted(self.sharp_files)\n        \n        print(f\"Found {len(self.sharp_files)} images in {self.sharp_dir}\")\n        \n        # Basic transforms\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n        \n        # Augmentation transforms for training\n        self.augment = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomVerticalFlip(),\n            transforms.RandomRotation(90),\n        ]) if split == 'train' else None\n        \n    def __len__(self):\n        return len(self.sharp_files)\n    \n    def get_random_crop_params(self, img):\n        \"\"\"Get random crop parameters\"\"\"\n        w, h = img.size\n        th, tw = self.patch_size, self.patch_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n        if w < tw or h < th:\n            # Handle images smaller than patch size by resizing\n            scale = max(tw / w, th / h) * 1.1  # Scale up with a small margin\n            new_w, new_h = int(w * scale), int(h * scale)\n            img = img.resize((new_w, new_h), Image.BICUBIC)\n            w, h = new_w, new_h\n        \n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw, img\n    \n    def __getitem__(self, idx):\n        try:\n            # Load sharp image\n            sharp_path = self.sharp_files[idx]\n            \n            # Get corresponding blur image with same name\n            file_name = sharp_path.name\n            blur_path = self.blur_dir / file_name\n            \n            # If blur file doesn't exist with exact name, try matching without extension\n            if not blur_path.exists():\n                stem = sharp_path.stem\n                for ext in self.extensions:\n                    candidate = self.blur_dir / f\"{stem}{ext}\"\n                    if candidate.exists():\n                        blur_path = candidate\n                        break\n                    \n                    # Also try with uppercase extension\n                    candidate = self.blur_dir / f\"{stem}{ext.upper()}\"\n                    if candidate.exists():\n                        blur_path = candidate\n                        break\n            \n            # If still no match, use a fallback\n            if not blur_path.exists():\n                print(f\"Warning: No matching blur image for {file_name}\")\n                # Return a random sample as fallback\n                return self.__getitem__(random.randint(0, len(self) - 1))\n            \n            # Open images with PIL\n            try:\n                sharp_img = Image.open(sharp_path).convert('RGB')\n                blur_img = Image.open(blur_path).convert('RGB')\n            except Exception as e:\n                print(f\"Error loading images: {e}\")\n                # Return a random sample as fallback\n                return self.__getitem__(random.randint(0, len(self) - 1))\n            \n            # Ensure both images have the same size\n            if sharp_img.size != blur_img.size:\n                blur_img = blur_img.resize(sharp_img.size, Image.BICUBIC)\n            \n            # Random crop for training\n            if self.split == 'train':\n                # Handle random cropping with potential resizing\n                i, j, h, w, sharp_img_resized = self.get_random_crop_params(sharp_img)\n                if sharp_img_resized is not sharp_img:  # If image was resized\n                    sharp_img = sharp_img_resized\n                    blur_img = blur_img.resize(sharp_img.size, Image.BICUBIC)\n                \n                # Crop both images to the same region\n                sharp_img = sharp_img.crop((j, i, j + w, i + h))\n                blur_img = blur_img.crop((j, i, j + w, i + h))\n                \n                # Apply augmentation\n                if random.random() > 0.5 and self.augment:\n                    state = torch.get_rng_state()\n                    sharp_img = self.augment(sharp_img)\n                    torch.set_rng_state(state)\n                    blur_img = self.augment(blur_img)\n            \n            # Convert to tensors\n            sharp_tensor = self.transform(sharp_img)\n            blur_tensor = self.transform(blur_img)\n            \n            return blur_tensor, sharp_tensor\n            \n        except Exception as e:\n            print(f\"Error processing image {idx}: {e}\")\n            # Return a random sample as fallback\n            return self.__getitem__(random.randint(0, len(self) - 1))\n\n\ndef create_dataloaders(root_dir_train, root_dir_val, batch_size=8, patch_size=256, num_workers=4):\n    \"\"\"Create training and validation dataloaders\"\"\"\n    train_dataset = DeblurDataset(root_dir_train, split='train', patch_size=patch_size)\n    val_dataset   = DeblurDataset(root_dir_val, split='train', patch_size=patch_size)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader","metadata":{"id":"PXE8qujKMgHZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{"id":"ZTkW2Gc1MgHd"}},{"cell_type":"code","source":"# utils/metrics.py\nimport torch\nimport lpips\nfrom pytorch_fid import fid_score\nimport pyiqa\nimport wandb\nfrom pytorch_msssim import ssim, ms_ssim\nfrom skimage.metrics import peak_signal_noise_ratio as f_psnr\n\nclass MetricsCalculator:\n    def __init__(self, device):\n        self.lpips_fn = lpips.LPIPS(net='vgg').to(device)\n        self.ssim = ssim\n        self.niqe = pyiqa.create_metric('niqe').to(device)\n\n    def calculate_metrics(self, pred, target):\n        with torch.no_grad():\n            psnr        = f_psnr(pred.detach().cpu().numpy(), target.detach().cpu().numpy(), data_range=1.0)\n            ssim        = self.ssim(pred, target).cpu()\n            lpips_value = self.lpips_fn(pred, target).mean().cpu()\n            #niqe_value  = self.niqe(pred.clip(0.0, 1.0)).mean().cpu()\n\n            return {\n                'psnr': psnr.item(),\n                'ssim': ssim.item(),\n                'lpips': lpips_value.item(),\n                'niqe': 0 #niqe_value.item()\n            }","metadata":{"id":"K0sVyXuVMgHe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"id":"4qRUao1IMgHf"}},{"cell_type":"code","source":"# train.py\nimport torch\nimport torch.nn as nn\nimport wandb\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nimport time\nfrom pathlib import Path\n\nclass Trainer:\n    def __init__(self, config):\n        self.config        = config\n        self.log_dir       = config.log_dir\n        self.generated_dir = config.generated_dir\n        self.device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Initialize models\n        self.netG = AFTNet().to(self.device)\n\n        # Initialize optimizers\n        self.optimG = torch.optim.AdamW(self.netG.parameters(), lr=config.lr, weight_decay=0.01, betas=(0.5, 0.999))\n\n        # Scheduler\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimG, T_0=10, T_mult=2)\n\n        self.com_criterion = DeblurLoss().to(self.device)\n\n        self.path = f\"{self.log_dir}/checkpoint_global.pt\"\n\n        print(\"Model's Total Num Model Parameters: {}\".format(sum([param.nelement() for param in self.netG.parameters()])))\n        model_size = get_model_size(self.netG)\n        print(f\"The model size is {model_size:.2f} MB\")\n\n        # Initialize metrics calculator\n        self.metrics = MetricsCalculator(self.device)\n\n        # Initialize wandb\n        #wandb.login(key=config.key)\n        #wandb.init(project=config.project_name, name=config.name, config=config.__dict__)\n\n    def train(self, train_loader, val_loader, resume_from=None):\n        # Resume if checkpoint provided\n        if self.config.resume and resume_from is not None:\n            start_epoch = self.load_checkpoint(resume_from)\n        else:\n            start_epoch = self.load_checkpoint(self.path)\n\n        print(f\"Resuming from step {self.path} with : (epoch {start_epoch})\")\n\n        try:\n            for epoch in range(start_epoch, self.config.epochs):\n                self.netG.train()\n                pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.epochs}')\n                for i, (blurred, target) in enumerate(pbar):\n                    blurred = blurred.float().to(self.device)\n                    target  = target.float().to(self.device)\n        \n                    # Train Generator\n                    self.optimG.zero_grad()\n                    outputs = self.netG(blurred)\n                    \n                    loss = self.com_criterion(outputs, target)\n                    loss.backward()\n                    \n                    # Gradient clipping\n                    torch.nn.utils.clip_grad_norm_(self.netG.parameters(), max_norm=1.0)\n                    \n                    self.optimG.step()\n                    \n                    metrics = self.metrics.calculate_metrics(outputs[0].detach(), target)\n                    # Update progress bar\n                    pbar.set_postfix({k: f'{v:.3f}' for k, v in metrics.items()})\n        \n                    if i % self.config.save_frequency == 0:\n                        # Save epoch checkpoint\n                        checkpoint_path = f\"{self.log_dir}/checkpoint_batch.pt\"\n                        self.save_checkpoint(checkpoint_path, epoch)\n\n                self.scheduler.step()\n\n                # Save epoch checkpoint\n                checkpoint_path = f\"{self.log_dir}/checkpoint_global.pt\"\n                self.save_checkpoint(checkpoint_path, epoch)\n                print(f\"Saved epoch checkpoint to {checkpoint_path}\")\n                \n                if epoch % self.config.val_frequency == 0:\n                    # Validation\n                    self.netG.eval()\n                    val_loss = 0\n                    with torch.no_grad():\n                        for blurred, target in val_loader:\n                            blurred, target = blurred.float().to(self.device), target.float().to(self.device)\n                            outputs = self.netG(blurred)\n                            val_loss += self.com_criterion(outputs, target).item()\n                    \n                    print(f'Epoch: {epoch}, Validation Loss: {val_loss/len(val_loader):.4f}')\n\n                if(epoch+1)%200==0:\n                  real_images, sharp_images = next(iter(val_loader))\n                  real_images = real_images.to(self.device)\n                  save_samples(self.netG, real_images, sharp_images, epoch+1, sample_dir=self.generated_dir)\n                  del real_images\n                  torch.cuda.empty_cache()\n\n        except KeyboardInterrupt:\n            print(\"Training interrupted by user\")\n            # Save interrupted checkpoint\n            checkpoint_path = f\"{self.log_dir}/checkpoint_interrupted.pt\"\n            self.save_checkpoint(checkpoint_path, epoch)\n            print(f\"Saved interrupt checkpoint to {checkpoint_path}\")\n        finally:\n            # Save final checkpoint\n            checkpoint_path = f\"{self.log_dir}/checkpoint_final.pt\"\n            self.save_checkpoint(checkpoint_path, self.config.epochs)\n            print(f\"Saved final checkpoint to {checkpoint_path}\")\n        \n    def log_metrics(self, loss_dict, metrics, epoch, iteration):\n        # Log losses\n        wandb.log({\n            'train/total_loss': loss_dict['total_g'],\n            'epoch': epoch,\n            'iteration': iteration\n        })\n\n        # Log metrics\n        wandb.log({\n            'train/psnr': metrics['psnr'],\n            'train/ssim': metrics['ssim'],\n            'train/lpips': metrics['lpips'],\n            'train/niqe': metrics['niqe']\n        })\n\n    def log_images(self, blurred, sharp, fake):\n        # Create image grid\n        img_grid = make_grid(torch.cat([\n            blurred, sharp, fake\n        ], dim=0), nrow=sharp.size(0), normalize=True, value_range=(-1, 1))\n\n        wandb.log({\n            'images': wandb.Image(img_grid, caption='Blurred | Sharp | Deblurred')\n        })\n\n    def log_validation_metrics(self, metrics, epoch):\n        wandb.log({\n            'val/psnr': metrics['psnr'],\n            'val/ssim': metrics['ssim'],\n            'val/lpips': metrics['lpips'],\n            'val/niqe': metrics['niqe'],\n            'epoch': epoch\n        })\n\n    def save_checkpoint(self, path: str, epoch: int):\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.netG.state_dict(),\n            'optim_state_dict': self.optimG.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n        }, path)\n\n    def load_checkpoint(self, path: str) -> int:\n        checkpoint = {}\n        checkpoint['epoch'] = 0\n        if self.config.resume and os.path.exists(path):\n            checkpoint = torch.load(path, map_location=self.device, weights_only=False)\n            self.netG.load_state_dict(checkpoint['model_state_dict'])\n            self.optimG.load_state_dict(checkpoint['optim_state_dict'])\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        return checkpoint['epoch']","metadata":{"id":"IzzBAXOqMgHg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{"id":"QmVsBP8-MgHj"}},{"cell_type":"code","source":"set_seed(42)\n\ncheckpoint_dir = './checkpoints'\nresults_dir = './results'\nsamples_dir = './samples'\ngenerated_dir = './generated'\nos.makedirs(samples_dir, exist_ok=True)\nos.makedirs(results_dir, exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)\nos.makedirs(generated_dir, exist_ok=True)\n\n\nconfig = type('Config', (), {\n    'project_name': 'aft-net',\n    'name': 'aft-net',\n    'lr': 2e-4,\n    'epochs': 2500,\n    'batch_size': 16,\n    'latent_dim': 8,\n    'lambda_l1': 10.0,\n    'lambda_kl': 0.01,\n    'lambda_tv': 0.1,\n    'lambda_adv': 0.01,\n    'log_frequency': 2500,\n    'val_frequency': 500,\n    'save_frequency': 500,\n    'key': '', # wandb key\n    'log_dir': checkpoint_dir,\n    'generated_dir': generated_dir,\n    'resume': True\n})()\n\ntrain_dir = ''\nval_dir   = ''\n\ntrain_loader, val_loader = create_dataloaders(train_dir, val_dir, batch_size=config.batch_size, patch_size=256, num_workers=4)\n\nplot_dataset(train_loader)\n\ntrainer = Trainer(config)\n\ntrainer.train(train_loader, val_loader)\ngenerator = trainer.netG","metadata":{"id":"gBDG-UuKMgHj","outputId":"6a67c666-c602-4bf3-e447-26724d0a06f7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"aYNvixMyMgHl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inferences","metadata":{"id":"vAZDb5bqMgHm"}},{"cell_type":"code","source":"def load_image(image_path):\n    # Load the image\n    image = Image.open(image_path)\n    image = transforms.ToTensor()(image)\n    image = image.unsqueeze(0)\n    return image\n\n\ndef infer(image_path, sharp_file, file_name, generator, out_name = '/kaggle/working/gen_results', device='cpu', timesteps=1000):\n\n    generator.to(device)\n    generator.eval()\n\n    # Load the image\n    image = load_image(image_path).to(device)\n    print(\"Processing image: \", image_path)\n    print(image.shape)\n    sr_imgs = generator(image)[0]\n    print(sr_imgs.shape)\n\n    # Save the image\n    # Save the image\n    save_image_tensor(image, out_name+f'normal_{file_name}')\n    save_image_tensor(sr_imgs, out_name+f'upsample_16_{file_name}')\n\n    hr_image = load_image(sharp_file).to(device)\n    print(hr_image.shape)\n\n    psnr, ssim = 0, 0 #calculate_metrics(hr_image, sr_imgs)\n    print(f\"PSNR: {psnr}, SSIM: {ssim}\")\n    # Display the original, compressed, and decompressed images\n    plt.subplot(1, 3, 1)\n    plt.imshow(get_pil_image(image.detach().cpu().squeeze(0)))\n    plt.title('Low Resolution Image')\n    plt.axis('off')\n    plt.subplot(1, 3, 2)\n    plt.imshow(get_pil_image(hr_image.detach().cpu().squeeze(0)))\n    plt.title('Original Image')\n    plt.axis('off')\n    plt.subplot(1, 3, 3)\n    plt.imshow(get_pil_image(sr_imgs.detach().cpu().squeeze(0)))\n    plt.title('UpScale Image')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    #plt.save(out_name+f'generated_{file_name}')\n    plt.close()\n    del hr_image, image, sr_imgs\n    torch.cuda.empty_cache()\n\n","metadata":{"id":"4KhdDcLpMgHn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    data_path = '/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Wider-Face/test/blur'\n    sharp_dir = '/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Wider-Face/test/sharp'\n    results_dir = 'samples/'\n    os.makedirs(results_dir, exist_ok=True)\n    list_of_files      = sorted(os.listdir(os.path.join(data_path)))\n    list_of_shap_files = sorted(os.listdir(os.path.join(sharp_dir)))\n    i = 0\n    for j, file in enumerate(list_of_files):\n        if file == '.ipynb_checkpoints':\n            continue\n        file_path  = data_path + '/' + file\n        sharp_path = sharp_dir + '/' + list_of_shap_files[i]\n        infer(file_path, sharp_path, file, generator, out_name=results_dir, device='cpu')\n        i = i+1\n        if i==25:\n            break","metadata":{"id":"_p-fr_JaMgHo","trusted":true},"outputs":[],"execution_count":null}]}