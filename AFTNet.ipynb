{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIoU2OFzMgG8",
    "outputId": "4191b285-5365-4ff3-edf2-1884d53ed86c",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install pytorch-msssim\n",
    "!pip install ptflops\n",
    "!pip install lpips\n",
    "!pip install --upgrade pip\n",
    "!pip install torch\n",
    "!pip install cv2\n",
    "!pip install scikit-image\n",
    "!pip install albumentations\n",
    "!pip install einops\n",
    "!pip install wandb\n",
    "!pip install torchmetrics\n",
    "!pip install pyiqa\n",
    "!pip install pytorch_fid\n",
    "!pip install piqa\n",
    "\n",
    "#!apt-get update && apt-get install libgl1 -y\n",
    "#!apt-get update && apt-get install -y python3-opencv\n",
    "#!pip install opencv-python\n",
    "from torch.nn.utils import spectral_norm\n",
    "from pytorch_msssim import ms_ssim, ssim, ssim as f_ssim\n",
    "from ptflops import get_model_complexity_info\n",
    "import segmentation_models_pytorch as smp\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torchvision.models import resnet50, densenet121, DenseNet121_Weights, ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import mean_squared_error as f_mse\n",
    "from skimage.metrics import peak_signal_noise_ratio as f_psnr\n",
    "from skimage.metrics import structural_similarity as f_ssim\n",
    "from skimage.metrics import normalized_root_mse as f_nrmse\n",
    "from skimage.metrics import normalized_mutual_information as f_nmi\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.models import vgg19, VGG19_Weights, VGG16_Weights\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from einops import rearrange, repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yE9i11sMgHJ",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AP98myAnMgHL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "class AdaptiveFeatureNorm(nn.Module):\n",
    "    \"\"\"Novel Adaptive Feature Normalization module with learnable statistics\"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super(AdaptiveFeatureNorm, self).__init__()\n",
    "        self.eps   = eps\n",
    "        self.gamma = Parameter(torch.ones(1, num_features, 1, 1))\n",
    "        self.beta  = Parameter(torch.zeros(1, num_features, 1, 1))\n",
    "        \n",
    "        # Adaptive statistics network\n",
    "        self.stats_net = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(num_features, num_features//4, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(num_features//4, num_features*2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        \n",
    "        # Calculate adaptive statistics\n",
    "        stats = self.stats_net(x)\n",
    "        adaptive_gamma, adaptive_beta = torch.chunk(stats, 2, dim=1)\n",
    "        \n",
    "        # Instance normalization\n",
    "        var, mean = torch.var_mean(x, dim=(2, 3), keepdim=True)\n",
    "        x_norm = (x - mean) / (var + self.eps).sqrt()\n",
    "        \n",
    "        # Apply adaptive scaling and shifting\n",
    "        return (1 + adaptive_gamma) * self.gamma * x_norm + adaptive_beta * self.beta\n",
    "\n",
    "class MultiScaleFrequencyAttention(nn.Module):\n",
    "    \"\"\"Novel multi-scale frequency attention module\"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super(MultiScaleFrequencyAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = dim ** -0.5\n",
    "        \n",
    "        self.qkv  = nn.Conv2d(dim, dim*3, 1)\n",
    "        self.proj = nn.Conv2d(dim, dim, 1)\n",
    "        \n",
    "        # Frequency decomposition branches\n",
    "        self.freq_decomp = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(dim, dim//4, 3, padding=1, groups=dim//4),\n",
    "                nn.GELU(),\n",
    "                nn.Conv2d(dim//4, dim, 1)\n",
    "            ) for _ in range(3)  # Low, mid, high frequencies\n",
    "        ])\n",
    "        \n",
    "        # Frequency attention weights\n",
    "        self.freq_weights = nn.Parameter(torch.ones(3))\n",
    "        self.softmax      = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Multi-head attention\n",
    "        qkv = self.qkv(x).reshape(B, 3, self.num_heads, C // self.num_heads, H, W)\n",
    "        q, k, v = qkv.unbind(1)\n",
    "        \n",
    "        # Attention computation\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # Frequency decomposition\n",
    "        freq_components = [decomp(x) for decomp in self.freq_decomp]\n",
    "        freq_weights = self.softmax(self.freq_weights)\n",
    "        \n",
    "        # Combine frequency components\n",
    "        freq_out = sum([w * f for w, f in zip(freq_weights, freq_components)])\n",
    "        \n",
    "        # Combine with spatial attention\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, C, H, W)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x + freq_out\n",
    "\n",
    "class TemporalConsistencyModule(nn.Module):\n",
    "    \"\"\"Novel temporal consistency module with adaptive feature alignment\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(TemporalConsistencyModule, self).__init__()\n",
    "        \n",
    "        # Feature alignment network\n",
    "        self.alignment_net = nn.Sequential(\n",
    "            nn.Conv2d(dim*2, dim//2, 3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim//2, 2, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attn = nn.Sequential(\n",
    "            nn.Conv2d(dim*2, dim//2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dim//2, dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.fusion = nn.Conv2d(dim*2, dim, 1)\n",
    "\n",
    "    def forward(self, current, previous):\n",
    "        # Calculate optical flow\n",
    "        flow = self.alignment_net(torch.cat([current, previous], dim=1))\n",
    "        \n",
    "        # Warp previous features\n",
    "        grid = self.get_grid(flow)\n",
    "        warped_prev = F.grid_sample(previous, grid, align_corners=True)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attn = self.temporal_attn(torch.cat([current, warped_prev], dim=1))\n",
    "        \n",
    "        # Feature fusion\n",
    "        fused = self.fusion(torch.cat([current * attn, warped_prev * (1-attn)], dim=1))\n",
    "        return fused.float()\n",
    "\n",
    "    def get_grid(self, flow):\n",
    "        B, _, H, W = flow.size()\n",
    "        xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n",
    "        yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n",
    "        xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        grid = torch.cat((xx,yy),1).float().to(flow.device)\n",
    "        vgrid = grid + flow\n",
    "        \n",
    "        # Scale grid to [-1,1]\n",
    "        vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:].clone()/max(W-1,1)-1.0\n",
    "        vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:].clone()/max(H-1,1)-1.0\n",
    "        return vgrid.permute(0,2,3,1)\n",
    "\n",
    "class AdaptiveResidualBlock(nn.Module):\n",
    "    \"\"\"Novel adaptive residual block with dynamic routing\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super(AdaptiveResidualBlock, self).__init__()\n",
    "        \n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim//4, 3, padding=1),\n",
    "            AdaptiveFeatureNorm(dim//4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(dim//4, dim, 3, padding=1),\n",
    "            AdaptiveFeatureNorm(dim)\n",
    "        )\n",
    "        \n",
    "        self.branch2 = MultiScaleFrequencyAttention(dim)\n",
    "        \n",
    "        # Dynamic routing network\n",
    "        self.router = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(dim, 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        route_weights = self.router(x)\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        \n",
    "        return x + route_weights[:,0:1,:,:] * out1 + route_weights[:,1:2,:,:] * out2\n",
    "\n",
    "def adjust(x1, x2):\n",
    "    x1 = F.interpolate(x1, size=x2.shape[2:], mode='nearest')\n",
    "    return x1\n",
    "\n",
    "class AFTNet(nn.Module):\n",
    "    \"\"\"Advanced Adaptive Frequency-Temporal Network for Image Deblurring\"\"\"\n",
    "    def __init__(self, in_channels=3, dim=32, num_blocks=4):\n",
    "        super(AFTNet, self).__init__()\n",
    "        \n",
    "        # Initial feature extraction\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, dim, 3, padding=1),\n",
    "            AdaptiveFeatureNorm(dim)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                AdaptiveResidualBlock(dim * (2**i)),\n",
    "                nn.Conv2d(dim * (2**i), dim * (2**(i+1)), 2, stride=2),\n",
    "                AdaptiveFeatureNorm(dim * (2**(i+1)))\n",
    "            ) for i in range(3)\n",
    "        ])\n",
    "\n",
    "        # bringing out prev feature to same level as the middle\n",
    "        self.conv_middle = nn.Conv2d(dim, dim*8, 1)\n",
    "        \n",
    "        # Middle blocks with temporal consistency\n",
    "        self.middle = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                AdaptiveResidualBlock(dim * 8),\n",
    "                TemporalConsistencyModule(dim * 8)\n",
    "            ) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.ConvTranspose2d(dim * (2**(i+1)), dim * (2**i), 2, stride=2),\n",
    "                AdaptiveFeatureNorm(dim * (2**i)),\n",
    "                AdaptiveResidualBlock(dim * (2**i))\n",
    "            ) for i in range(2, -1, -1)\n",
    "        ])\n",
    "        \n",
    "        # Feature pyramid fusion\n",
    "        self.pyramid_fusion = nn.ModuleList([\n",
    "            nn.Conv2d(dim * (2**i) * 2, dim * (2**i), 1)\n",
    "            for i in range(3)\n",
    "        ])\n",
    "\n",
    "        # Multi-scale output\n",
    "        self.output_layers = nn.ModuleList([\n",
    "            nn.Conv2d(dim * (2**i), in_channels, 3, padding=1)\n",
    "            for i in range(4)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, prev_frame=None):\n",
    "        r = x\n",
    "        if prev_frame is None:\n",
    "            prev_frame = x\n",
    "            \n",
    "        # Initial features\n",
    "        x = self.init_conv(x)\n",
    "        with torch.no_grad():\n",
    "            prev_features = self.init_conv(prev_frame)\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_features = [x]\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            encoder_features.append(x)\n",
    "\n",
    "        prev_features = F.interpolate(prev_features, size=x.shape[2:], mode='bicubic', align_corners=False)\n",
    "        prev_features = self.conv_middle(prev_features)\n",
    "\n",
    "        # Middle blocks with temporal consistency\n",
    "        for block in self.middle:\n",
    "            x = block[0](x)                 # Residual block\n",
    "            x = block[1](x, prev_features)  # Temporal consistency\n",
    "            prev_features = x\n",
    "\n",
    "        # Multi-scale outputs\n",
    "        outputs = [(self.output_layers[-1](x)+F.interpolate(r, size=x.shape[2:], mode='bicubic', align_corners=False)).clamp(0, 1)] \n",
    "        \n",
    "        # Decoder with feature pyramid fusion\n",
    "        for i, dec in enumerate(self.decoder):\n",
    "            # Upsample current features\n",
    "            x = dec[0](x)  # Upsample\n",
    "            \n",
    "            # Fusion with encoder features\n",
    "            s = encoder_features[::-1][1:][i]\n",
    "            x = adjust(x, s)\n",
    "            x = self.pyramid_fusion[::-1][i](torch.cat([x, s], dim=1))\n",
    "            \n",
    "            # Apply remaining decoder operations\n",
    "            x = dec[1:](x).float()\n",
    "            \n",
    "            # Generate output at current scale\n",
    "            outputs.append((self.output_layers[-(i+2)](x)+F.interpolate(r, size=x.shape[2:], mode='bicubic', align_corners=False)).clamp(0, 1))\n",
    "        \n",
    "        return outputs[::-1]  # Return multi-scale outputs from fine to coarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTgjyCmbMgHP"
   },
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25bASKFlMgHS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# models/discriminator.py\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, ndf=64, n_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        for i in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**i, 8)\n",
    "            model += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "        model += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7dfD2MyQYS_"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoG5ZrDwQa2z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.fft import fft2, ifft2\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-3):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        diff = pred - target\n",
    "        loss = torch.mean(Variable(torch.sqrt(diff * diff + self.epsilon * self.epsilon).type(torch.FloatTensor), requires_grad=True))\n",
    "        return loss\n",
    "\n",
    "class MSEGDL(nn.Module):\n",
    "    def __init__(self, lambda_mse=1, lambda_gdl=1):\n",
    "        super(MSEGDL, self).__init__()\n",
    "        self.lambda_mse = lambda_mse\n",
    "        self.lambda_gdl = lambda_gdl\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        squared_error = (inputs - targets).pow(2)\n",
    "        gradient_diff_i = (inputs.diff(axis=-1)-targets.diff(axis=-1)).pow(2)\n",
    "        gradient_diff_j =  (inputs.diff(axis=-2)-targets.diff(axis=-2)).pow(2)\n",
    "        loss = (self.lambda_mse*squared_error.sum() + self.lambda_gdl*gradient_diff_i.sum() + self.lambda_gdl*gradient_diff_j.sum())/inputs.numel()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, size_average=True):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.data_range = data_range\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - Variable(ssim(img1, img2, data_range=self.data_range, size_average=self.size_average).type(torch.FloatTensor), requires_grad=True)\n",
    "\n",
    "class MSSSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, size_average=True):\n",
    "        super(MSSSIMLoss, self).__init__()\n",
    "        self.data_range = data_range\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - Variable(ms_ssim(img1, img2, data_range=self.data_range, size_average=self.size_average).type(torch.FloatTensor), requires_grad=True)\n",
    "\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, layer=36):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:layer].eval()\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        self.vgg.eval()\n",
    "        vgg_input_features = self.vgg(output)\n",
    "        vgg_target_features = self.vgg(target)\n",
    "        loss = self.loss(vgg_input_features, vgg_target_features)\n",
    "        del vgg_input_features, vgg_target_features\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "class DeblurLoss(nn.Module):\n",
    "    \"\"\"Advanced loss function combining multiple objectives\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeblurLoss, self).__init__()\n",
    "        self.l1_loss   = nn.L1Loss()\n",
    "        self.mse_loss  = nn.MSELoss()\n",
    "        self.gdl_loss  = MSEGDL()\n",
    "        self.ssim_loss = SSIMLoss()\n",
    "        self.vgg_loss  = VGGLoss()\n",
    "            \n",
    "    def get_frequency_loss(self, pred, target):\n",
    "        # FFT-based frequency loss\n",
    "        pred_freq = torch.fft.fft2(pred)\n",
    "        target_freq = torch.fft.fft2(target)\n",
    "        return F.mse_loss(pred_freq.abs(), target_freq.abs())\n",
    "\n",
    "    def forward(self, pred_list, target):\n",
    "        total_loss = 0\n",
    "        weights = [1.0, 0.75, 0.45, 0.35]  # Weights for different scales\n",
    "        \n",
    "        for pred, weight in zip(pred_list, weights):\n",
    "            # Resize target to match prediction if needed\n",
    "            if pred.shape != target.shape:\n",
    "                target_resized = F.interpolate(target, size=pred.shape[2:]).to(target)\n",
    "            else:\n",
    "                target_resized = target\n",
    "\n",
    "            pred = pred.to(target)\n",
    "            # Pixel loss\n",
    "            pixel_loss = self.l1_loss(pred, target_resized)\n",
    "            \n",
    "            # Frequency loss\n",
    "            freq_loss = self.get_frequency_loss(pred, target_resized)\n",
    "            \n",
    "            # Perceptual loss\n",
    "            perc_loss = self.vgg_loss(pred, target_resized)\n",
    "\n",
    "            # SSIM loss\n",
    "            ssim_loss = self.ssim_loss(pred, target_resized)\n",
    "\n",
    "            # GDL loss\n",
    "            gdl_loss = self.gdl_loss(pred, target_resized)\n",
    "            \n",
    "            # Combine losses with weights\n",
    "            total_loss += weight * (\n",
    "                1.0 * pixel_loss + \n",
    "                0.6 * ssim_loss +\n",
    "                0.3 * gdl_loss +\n",
    "                0.1 * freq_loss + \n",
    "                0.8 * perc_loss\n",
    "            )\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZX9jDncMgHU"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OGwgwveMgHV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Training and Validation Functions\n",
    "def calculate_metrics(pred, target):\n",
    "    \"\"\"Calculate PSNR and SSIM metrics\"\"\"\n",
    "    mse = F.mse_loss(pred, target)\n",
    "    psnr = 10 * torch.log10(1 / mse)\n",
    "    ssim_value = ssim(pred, target, data_range=1.0, size_average=True)\n",
    "    return psnr.item(), ssim_value.item()\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculates the size of a PyTorch model in megabytes (MB).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to calculate the size for.\n",
    "\n",
    "    Returns:\n",
    "        float: The size of the model in megabytes (MB).\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    total_size = (param_size + buffer_size) / 1024**2\n",
    "    return total_size\n",
    "\n",
    "def plot_dataset(train_loader):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(14, 7))\n",
    "\n",
    "    for i, (low_res, high_res) in enumerate(train_loader):\n",
    "        if i >= 5:\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "        axes[0, i].imshow(low_res[0].permute(1, 2, 0))\n",
    "        axes[0, i].set_title(\"Low Resolution\")\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        axes[1, i].imshow(high_res[0].permute(1, 2, 0))\n",
    "        axes[1, i].set_title(\"High Resolution\")\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "def get_pil_image(image_tensor):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        transforms.Lambda(lambda t: t*255.),\n",
    "        transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    return transform(image_tensor)\n",
    "\n",
    "def save_image_tensor(tensor_image, image_name):\n",
    "  # Convert the tensor image to a PIL image\n",
    "  pil_image = get_pil_image(tensor_image.squeeze(0))\n",
    "  # Save the PIL image\n",
    "  pil_image.save(image_name)\n",
    "\n",
    "def save_pil_image(image, image_name):\n",
    "    image.save(image_name)\n",
    "\n",
    "def save_samples(encoder, real_images, sharp_images, index, sample_dir='generated', show=True, device='cuda'):\n",
    "  with torch.no_grad():\n",
    "    #Sample random style code\n",
    "    fake_images = encoder(real_images)[0]\n",
    "    fake_name   = \"generated-images-{0:0=4d}.png\".format(index)\n",
    "    save_image(fake_images, os.path.join(sample_dir, fake_name), nrow=8)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(sharp_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
    "        plt.show()\n",
    "\n",
    "def show_images(images):\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.imshow(make_grid(images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculates the size of a PyTorch model in megabytes (MB).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to calculate the size for.\n",
    "\n",
    "    Returns:\n",
    "        float: The size of the model in megabytes (MB).\n",
    "    \"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    total_size = (param_size + buffer_size) / 1024**2\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEI7v2pVMgHY"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXE8qujKMgHZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class SuperLowDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', patch_size=256, scale_factor=4):\n",
    "        \"\"\"\n",
    "        Dataset for super-resolution training/validation\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Root directory containing high-resolution images\n",
    "            split: 'train' or 'val'\n",
    "            patch_size: Size of high-resolution training patches (only used during training)\n",
    "            scale_factor: Downsampling factor for creating low-resolution images\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.patch_size = patch_size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.lr_patch_size = patch_size // scale_factor\n",
    "        \n",
    "        # Get high-resolution images with multiple extensions\n",
    "        self.hr_dir = self.root_dir\n",
    "        \n",
    "        # Common image extensions\n",
    "        self.extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp']\n",
    "        \n",
    "        # Get all high-resolution images\n",
    "        self.hr_files = []\n",
    "        for ext in self.extensions:\n",
    "            self.hr_files.extend(list(self.hr_dir.glob(f'*{ext}')))\n",
    "            self.hr_files.extend(list(self.hr_dir.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        # Sort the files to ensure deterministic behavior\n",
    "        self.hr_files = sorted(self.hr_files)\n",
    "        \n",
    "        print(f\"Found {len(self.hr_files)} high-resolution images in {self.hr_dir}\")\n",
    "        \n",
    "        # Basic transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "        # Augmentation transforms for training\n",
    "        self.augment = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(90),\n",
    "        ]) if split == 'train' else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_files)\n",
    "    \n",
    "    def get_random_crop_params(self, img):\n",
    "        \"\"\"Get random crop parameters for high-resolution image\"\"\"\n",
    "        w, h = img.size\n",
    "        th, tw = self.patch_size, self.patch_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "        if w < tw or h < th:\n",
    "            # Handle images smaller than patch size by resizing\n",
    "            scale = max(tw / w, th / h) * 1.1  # Scale up with a small margin\n",
    "            new_w, new_h = int(w * scale), int(h * scale)\n",
    "            img = img.resize((new_w, new_h), Image.BICUBIC)\n",
    "            w, h = new_w, new_h\n",
    "        \n",
    "        i = random.randint(0, h - th)\n",
    "        j = random.randint(0, w - tw)\n",
    "        return i, j, th, tw, img\n",
    "    \n",
    "    def create_low_res(self, hr_img):\n",
    "        \"\"\"Create low-resolution image by downscaling with bicubic interpolation\"\"\"\n",
    "        w, h = hr_img.size\n",
    "        lr_w, lr_h = w // self.scale_factor, h // self.scale_factor\n",
    "        lr_img = hr_img.resize((lr_w, lr_h), Image.BICUBIC)\n",
    "        return lr_img\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load high-resolution image\n",
    "            hr_path = self.hr_files[idx]\n",
    "            \n",
    "            # Open image with PIL\n",
    "            try:\n",
    "                hr_img = Image.open(hr_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {e}\")\n",
    "                # Return a random sample as fallback\n",
    "                return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "            \n",
    "            # Random crop for training\n",
    "            if self.split == 'train':\n",
    "                # Handle random cropping with potential resizing\n",
    "                i, j, h, w, hr_img_resized = self.get_random_crop_params(hr_img)\n",
    "                if hr_img_resized is not hr_img:  # If image was resized\n",
    "                    hr_img = hr_img_resized\n",
    "                \n",
    "                # Crop high-resolution image\n",
    "                hr_img = hr_img.crop((j, i, j + w, i + h))\n",
    "                \n",
    "                # Apply augmentation\n",
    "                if random.random() > 0.5 and self.augment:\n",
    "                    hr_img = self.augment(hr_img)\n",
    "            \n",
    "            # Create low-resolution version\n",
    "            lr_img = self.create_low_res(hr_img)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            hr_tensor = self.transform(hr_img)\n",
    "            lr_tensor = self.transform(lr_img)\n",
    "            \n",
    "            return lr_tensor, hr_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {idx}: {e}\")\n",
    "            # Return a random sample as fallback\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "\n",
    "def create_dataloaders(root_dir_train, root_dir_val, batch_size=8, patch_size=256, scale_factor=4, num_workers=4):\n",
    "    \"\"\"Create training and validation dataloaders for super-resolution\"\"\"\n",
    "    train_dataset = SuperLowDataset(root_dir_train, split='train', patch_size=patch_size, scale_factor=scale_factor)\n",
    "    val_dataset = SuperLowDataset(root_dir_val, split='train', patch_size=patch_size, scale_factor=scale_factor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTkW2Gc1MgHd"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0sVyXuVMgHe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# utils/metrics.py\n",
    "import torch\n",
    "import lpips\n",
    "from pytorch_fid import fid_score\n",
    "import pyiqa\n",
    "import wandb\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as f_psnr\n",
    "\n",
    "class MetricsCalculator:\n",
    "    def __init__(self, device):\n",
    "        self.lpips_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "        self.ssim = ssim\n",
    "        self.niqe = pyiqa.create_metric('niqe').to(device)\n",
    "\n",
    "    def calculate_metrics(self, pred, target):\n",
    "        with torch.no_grad():\n",
    "            psnr        = f_psnr(pred.detach().cpu().numpy(), target.detach().cpu().numpy(), data_range=1.0)\n",
    "            ssim        = self.ssim(pred, target).cpu()\n",
    "            lpips_value = self.lpips_fn(pred, target).mean().cpu()\n",
    "            #niqe_value  = self.niqe(pred.clip(0.0, 1.0)).mean().cpu()\n",
    "\n",
    "            return {\n",
    "                'psnr': psnr.item(),\n",
    "                'ssim': ssim.item(),\n",
    "                'lpips': lpips_value.item(),\n",
    "                'niqe': 0 #niqe_value.item()\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qRUao1IMgHf"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzzBAXOqMgHg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config        = config\n",
    "        self.log_dir       = config.log_dir\n",
    "        self.generated_dir = config.generated_dir\n",
    "        self.device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize models\n",
    "        self.netG = AFTNet().to(self.device)\n",
    "\n",
    "        # Initialize optimizers\n",
    "        self.optimG = torch.optim.AdamW(self.netG.parameters(), lr=config.lr, weight_decay=0.01, betas=(0.5, 0.999))\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimG, T_0=10, T_mult=2)\n",
    "\n",
    "        self.com_criterion = DeblurLoss().to(self.device)\n",
    "\n",
    "        self.path = f\"{self.log_dir}/checkpoint_global.pt\"\n",
    "\n",
    "        print(\"Model's Total Num Model Parameters: {}\".format(sum([param.nelement() for param in self.netG.parameters()])))\n",
    "        model_size = get_model_size(self.netG)\n",
    "        print(f\"The model size is {model_size:.2f} MB\")\n",
    "\n",
    "        # Initialize metrics calculator\n",
    "        self.metrics = MetricsCalculator(self.device)\n",
    "\n",
    "        # Initialize wandb\n",
    "        #wandb.login(key=config.key)\n",
    "        #wandb.init(project=config.project_name, name=config.name, config=config.__dict__)\n",
    "\n",
    "    def train(self, train_loader, val_loader, resume_from=None):\n",
    "        # Resume if checkpoint provided\n",
    "        if self.config.resume and resume_from is not None:\n",
    "            start_epoch = self.load_checkpoint(resume_from)\n",
    "        else:\n",
    "            start_epoch = self.load_checkpoint(self.path)\n",
    "\n",
    "        print(f\"Resuming from step {self.path} with : (epoch {start_epoch})\")\n",
    "\n",
    "        try:\n",
    "            for epoch in range(start_epoch, self.config.epochs):\n",
    "                self.netG.train()\n",
    "                pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.epochs}')\n",
    "                for i, (blurred, target) in enumerate(pbar):\n",
    "                    blurred = blurred.float().to(self.device)\n",
    "                    target  = target.float().to(self.device)\n",
    "        \n",
    "                    # Train Generator\n",
    "                    self.optimG.zero_grad()\n",
    "                    outputs = self.netG(blurred)\n",
    "                    \n",
    "                    loss = self.com_criterion(outputs, target)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(self.netG.parameters(), max_norm=1.0)\n",
    "                    \n",
    "                    self.optimG.step()\n",
    "                    \n",
    "                    metrics = self.metrics.calculate_metrics(outputs[0].detach(), target)\n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({k: f'{v:.3f}' for k, v in metrics.items()})\n",
    "        \n",
    "                    if i % self.config.save_frequency == 0:\n",
    "                        # Save epoch checkpoint\n",
    "                        checkpoint_path = f\"{self.log_dir}/checkpoint_batch.pt\"\n",
    "                        self.save_checkpoint(checkpoint_path, epoch)\n",
    "\n",
    "                self.scheduler.step()\n",
    "\n",
    "                # Save epoch checkpoint\n",
    "                checkpoint_path = f\"{self.log_dir}/checkpoint_global.pt\"\n",
    "                self.save_checkpoint(checkpoint_path, epoch)\n",
    "                print(f\"Saved epoch checkpoint to {checkpoint_path}\")\n",
    "                \n",
    "                if epoch % self.config.val_frequency == 0:\n",
    "                    # Validation\n",
    "                    self.netG.eval()\n",
    "                    val_loss = 0\n",
    "                    with torch.no_grad():\n",
    "                        for blurred, target in val_loader:\n",
    "                            blurred, target = blurred.float().to(self.device), target.float().to(self.device)\n",
    "                            outputs = self.netG(blurred)\n",
    "                            val_loss += self.com_criterion(outputs, target).item()\n",
    "                    \n",
    "                    print(f'Epoch: {epoch}, Validation Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "                if(epoch+1)%200==0:\n",
    "                  real_images, sharp_images = next(iter(val_loader))\n",
    "                  real_images = real_images.to(self.device)\n",
    "                  save_samples(self.netG, real_images, sharp_images, epoch+1, sample_dir=self.generated_dir)\n",
    "                  del real_images\n",
    "                  torch.cuda.empty_cache()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted by user\")\n",
    "            # Save interrupted checkpoint\n",
    "            checkpoint_path = f\"{self.log_dir}/checkpoint_interrupted.pt\"\n",
    "            self.save_checkpoint(checkpoint_path, epoch)\n",
    "            print(f\"Saved interrupt checkpoint to {checkpoint_path}\")\n",
    "        finally:\n",
    "            # Save final checkpoint\n",
    "            checkpoint_path = f\"{self.log_dir}/checkpoint_final.pt\"\n",
    "            self.save_checkpoint(checkpoint_path, self.config.epochs)\n",
    "            print(f\"Saved final checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "    def log_metrics(self, loss_dict, metrics, epoch, iteration):\n",
    "        # Log losses\n",
    "        wandb.log({\n",
    "            'train/total_loss': loss_dict['total_g'],\n",
    "            'epoch': epoch,\n",
    "            'iteration': iteration\n",
    "        })\n",
    "\n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "            'train/psnr': metrics['psnr'],\n",
    "            'train/ssim': metrics['ssim'],\n",
    "            'train/lpips': metrics['lpips'],\n",
    "            'train/niqe': metrics['niqe']\n",
    "        })\n",
    "\n",
    "    def log_images(self, blurred, sharp, fake):\n",
    "        # Create image grid\n",
    "        img_grid = make_grid(torch.cat([\n",
    "            blurred, sharp, fake\n",
    "        ], dim=0), nrow=sharp.size(0), normalize=True, value_range=(-1, 1))\n",
    "\n",
    "        wandb.log({\n",
    "            'images': wandb.Image(img_grid, caption='Blurred | Sharp | Deblurred')\n",
    "        })\n",
    "\n",
    "    def log_validation_metrics(self, metrics, epoch):\n",
    "        wandb.log({\n",
    "            'val/psnr': metrics['psnr'],\n",
    "            'val/ssim': metrics['ssim'],\n",
    "            'val/lpips': metrics['lpips'],\n",
    "            'val/niqe': metrics['niqe'],\n",
    "            'epoch': epoch\n",
    "        })\n",
    "\n",
    "    def save_checkpoint(self, path: str, epoch: int):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.netG.state_dict(),\n",
    "            'optim_state_dict': self.optimG.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "    def load_checkpoint(self, path: str) -> int:\n",
    "        checkpoint = {}\n",
    "        checkpoint['epoch'] = 0\n",
    "        if self.config.resume and os.path.exists(path):\n",
    "            checkpoint = torch.load(path, map_location=self.device, weights_only=False)\n",
    "            self.netG.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.optimG.load_state_dict(checkpoint['optim_state_dict'])\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        return checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmVsBP8-MgHj"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBDG-UuKMgHj",
    "outputId": "6a67c666-c602-4bf3-e447-26724d0a06f7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "results_dir = './results'\n",
    "samples_dir = './samples'\n",
    "generated_dir = './generated'\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(generated_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "config = type('Config', (), {\n",
    "    'project_name': 'aft-net',\n",
    "    'name': 'aft-net',\n",
    "    'lr': 2e-4,\n",
    "    'epochs': 2500,\n",
    "    'batch_size': 16,\n",
    "    'latent_dim': 8,\n",
    "    'lambda_l1': 10.0,\n",
    "    'lambda_kl': 0.01,\n",
    "    'lambda_tv': 0.1,\n",
    "    'lambda_adv': 0.01,\n",
    "    'log_frequency': 2500,\n",
    "    'val_frequency': 500,\n",
    "    'save_frequency': 500,\n",
    "    'key': '', # wandb key\n",
    "    'log_dir': checkpoint_dir,\n",
    "    'generated_dir': generated_dir,\n",
    "    'resume': True\n",
    "})()\n",
    "\n",
    "train_dir = ''\n",
    "val_dir   = ''\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(train_dir, val_dir, batch_size=config.batch_size, patch_size=256, num_workers=4)\n",
    "\n",
    "plot_dataset(train_loader)\n",
    "\n",
    "trainer = Trainer(config)\n",
    "\n",
    "trainer.train(train_loader, val_loader)\n",
    "generator = trainer.netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYNvixMyMgHl",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAZDb5bqMgHm"
   },
   "source": [
    "# Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KhdDcLpMgHn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def infer(image_path, sharp_file, file_name, generator, out_name = '/kaggle/working/gen_results', device='cpu', timesteps=1000):\n",
    "\n",
    "    generator.to(device)\n",
    "    generator.eval()\n",
    "\n",
    "    # Load the image\n",
    "    image = load_image(image_path).to(device)\n",
    "    print(\"Processing image: \", image_path)\n",
    "    print(image.shape)\n",
    "    sr_imgs = generator(image)[0]\n",
    "    print(sr_imgs.shape)\n",
    "\n",
    "    # Save the image\n",
    "    # Save the image\n",
    "    save_image_tensor(image, out_name+f'normal_{file_name}')\n",
    "    save_image_tensor(sr_imgs, out_name+f'upsample_16_{file_name}')\n",
    "\n",
    "    hr_image = load_image(sharp_file).to(device)\n",
    "    print(hr_image.shape)\n",
    "\n",
    "    psnr, ssim = 0, 0 #calculate_metrics(hr_image, sr_imgs)\n",
    "    print(f\"PSNR: {psnr}, SSIM: {ssim}\")\n",
    "    # Display the original, compressed, and decompressed images\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(get_pil_image(image.detach().cpu().squeeze(0)))\n",
    "    plt.title('Low Resolution Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(get_pil_image(hr_image.detach().cpu().squeeze(0)))\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(get_pil_image(sr_imgs.detach().cpu().squeeze(0)))\n",
    "    plt.title('UpScale Image')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #plt.save(out_name+f'generated_{file_name}')\n",
    "    plt.close()\n",
    "    del hr_image, image, sr_imgs\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p-fr_JaMgHo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data_path = '/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Wider-Face/test/blur'\n",
    "    sharp_dir = '/kaggle/input/a-curated-list-of-image-deblurring-datasets/DBlur/Wider-Face/test/sharp'\n",
    "    results_dir = 'samples/'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    list_of_files      = sorted(os.listdir(os.path.join(data_path)))\n",
    "    list_of_shap_files = sorted(os.listdir(os.path.join(sharp_dir)))\n",
    "    i = 0\n",
    "    for j, file in enumerate(list_of_files):\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        file_path  = data_path + '/' + file\n",
    "        sharp_path = sharp_dir + '/' + list_of_shap_files[i]\n",
    "        infer(file_path, sharp_path, file, generator, out_name=results_dir, device='cpu')\n",
    "        i = i+1\n",
    "        if i==25:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 270005,
     "sourceId": 579020,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 627736,
     "sourceId": 1118216,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1899282,
     "sourceId": 3111719,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1964876,
     "sourceId": 3241736,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2305649,
     "sourceId": 3879819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3055596,
     "sourceId": 5251537,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
